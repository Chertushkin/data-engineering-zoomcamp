import io
import os
import torch
import logging
import numpy as np
import pandas as pd
import glob
import json
import torch.nn.functional as F
from transformers import BertForSequenceClassification, BertTokenizer
from transformers import AutoModelForSequenceClassification, AutoTokenizer

max_len = int(os.getenv("MAX_LEN"))
content_type = os.getenv("CONTENT_TYPE")
pretrained_model_name = os.getenv("PRETRAINED_MODEL_NAME")
class_names = json.loads(os.getenv("CLASS_NAMES"))


def load_model_from_pth(model_path):
    # load tokenizer
    logging.warning(f"Loading from pth: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name, do_lower_case=True, force_download=True
    )

    device = torch.device(
        "cuda:" + str(torch.cuda.current_device())
        if torch.cuda.is_available()
        else "cpu"
    )

    # load trained weights
    model_state_dict = torch.load(model_path, map_location=device)

    # load BERT intial model with trained weights
    model = AutoModelForSequenceClassification.from_pretrained(
        pretrained_model_name, state_dict=model_state_dict, force_download=True,
    )

    return model, tokenizer


def model_fn(model_dir):
    """
    Loads model and its weights,and return the loaded model
    for inference
    Args:
        model_dir: the path to the S3 bucket containing the model file
    Returns:
        loaded model transferred to the appropriate device
    """

    logging.info("in model_fn()")

    # compute processor
    device = torch.device(
        "cuda:" + str(torch.cuda.current_device())
        if torch.cuda.is_available()
        else "cpu"
    )

    pths = glob.glob(model_dir + "/*.pth")

    logging.info(f"Loading model from {model_dir}")

    if len(pths) == 1:
        model, tokenizer = load_model_from_pth(pths[0])
    else:
        model = AutoModelForSequenceClassification.from_pretrained(model_dir)
        tokenizer = None
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_dir)
        except EnvironmentError as e:
            logging.error(
                f"{model_dir} does not have pretrained Tokenizer. Fallback to standard..."
            )
        finally:
            if tokenizer is None:
                tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)

    model = model.to(device)
    return model, tokenizer


def input_fn(serialized_input_data, input_content_type):
    """
    Takes request data and deserialises the data into an object for prediction
    Args:
        request_body: a byte buffer array
        content_type: a python string, written as "application/x"
        where x is the content type csv.
    Returns:
        dataframe series
    """

    logging.info("in input_fn()")

    # validate input content type
    if input_content_type == content_type:
        df = pd.read_csv(io.StringIO(serialized_input_data), dtype=str, header=None)[0]
        logging.info(f"Successfully read csv, payload item {df}")
        return df
    else:
        raise ValueError(f"Unsupported content type:{content_type}")


def predict_fn(input_data, model_artifacts):
    """
    Customises how the model server gets predictions from the loaded model
    Args:
        input_data:data loaded via input_fn()(i.e. each row)
        model: model loaded via model_fn() above
    Returns:
        a numpy array (2D) where each row is an entry
    """

    logging.info("in predict_fn()")

    input_ids = []
    attention_masks = []
    model, tokenizer = model_artifacts

    # limit number of CPU threads to be used per worker
    torch.set_num_threads(1)

    # encode input text
    model.eval()
    with torch.no_grad():
        for row in input_data:
            encoded_row = tokenizer.encode_plus(
                text=row,
                add_special_tokens=True,
                max_length=max_len,
                pad_to_max_length=True,
                return_attention_mask=True,
                truncation=True,
            )
            input_ids.append(encoded_row.get("input_ids"))
            attention_masks.append(encoded_row.get("attention_mask"))

        # return the class with the highest prob with corresponding index
        input_ids = torch.tensor(input_ids)
        attention_masks = torch.tensor(attention_masks)
        logits = model(input_ids, attention_masks)
        probs = F.softmax(logits[0], dim=1)
        logging.info("Prediction done...")
        return np.array(probs)


def output_fn(prediction_output, accept):
    """
    Serializes the prediction result into the desired response content type
    Args:
        predictions: a list of predictions generated by predict_fn()
    Returns:
        an output list, saved with the .out extension in an S3 bucket
    """

    logging.info("in output_fn()")

    final_predictions = []

    # associate class with predictions
    for prediction in prediction_output:
        final_predictions.append(
            (
                class_names[prediction.argmax()],
                np.max(prediction),
                prediction[0],
                prediction[1],
                prediction[2],
            )
        )

    logging.info(f"Generated prediction {final_predictions}")

    return pd.DataFrame(final_predictions).to_csv(index=False, header=None)
